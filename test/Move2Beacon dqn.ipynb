{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.4\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.lib import actions\n",
    "from pysc2.lib import features\n",
    "from pysc2.env import sc2_env, run_loop, available_actions_printer\n",
    "from pysc2 import maps\n",
    "from absl import flags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv1D,Dropout,Flatten,Activation,MaxPool1D,MaxPool2D\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the flags for the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_AI_RELATIVE = features.SCREEN_FEATURES.player_relative.index\n",
    "_AI_SELECTED = features.SCREEN_FEATURES.selected.index\n",
    "_NO_OP = actions.FUNCTIONS.no_op.id\n",
    "_MOVE_SCREEN = actions.FUNCTIONS.Attack_screen.id\n",
    "_SELECT_ARMY = actions.FUNCTIONS.select_army.id\n",
    "_SELECT_POINT = actions.FUNCTIONS.select_point.id\n",
    "_MOVE_RAND = 1000\n",
    "_MOVE_MIDDLE = 2000\n",
    "_BACKGROUND = 0\n",
    "_AI_SELF = 1\n",
    "_AI_ALLIES = 2\n",
    "_AI_NEUTRAL = 3\n",
    "_AI_HOSTILE = 4\n",
    "_SELECT_ALL = [0]\n",
    "_NOT_QUEUED = [0]\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.025\n",
    "EPS_DECAY = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our actions\n",
    "# it can choose to move to\n",
    "# the beacon or to do nothing\n",
    "# it can select the marine or deselect\n",
    "# the marine, it can move to a random point\n",
    "possible_actions = [\n",
    "    _NO_OP,\n",
    "    _SELECT_ARMY,\n",
    "    _SELECT_POINT,\n",
    "    _MOVE_SCREEN,\n",
    "    _MOVE_RAND,\n",
    "    _MOVE_MIDDLE\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state(obs):\n",
    "    ai_view = obs.observation['screen'][_AI_RELATIVE]\n",
    "    beaconxs, beaconys = (ai_view == _AI_NEUTRAL).nonzero()\n",
    "    marinexs, marineys = (ai_view == _AI_SELF).nonzero()\n",
    "    marinex, mariney = marinexs.mean(), marineys.mean()\n",
    "        \n",
    "    marine_on_beacon = np.min(beaconxs) <= marinex <=  np.max(beaconxs) and np.min(beaconys) <= mariney <=  np.max(beaconys)\n",
    "        \n",
    "    # get a 1 or 0 for whether or not our marine is selected\n",
    "    ai_selected = obs.observation['screen'][_AI_SELECTED]\n",
    "    marine_selected = int((ai_selected == 1).any())\n",
    "    return ai_view,marine_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 64, 64)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 57, 16)       8208        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 57, 16)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 28, 16)       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 25, 32)       2080        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 25, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 6, 32)        0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 6, 32)        0           max_pooling1d_2[0][0]            \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6, 6)         198         add_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 10,486\n",
      "Trainable params: 10,486\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = keras.layers.Input(shape=(64,64))\n",
    "model_view = Conv1D(16, kernel_size=(8,), input_shape=(64,64))(input1)\n",
    "model_view = Activation('relu')(model_view)\n",
    "model_view = MaxPool1D(pool_size=(2,), strides=(2,), padding='valid')(model_view)\n",
    "model_view = Conv1D(32, kernel_size=(4, ), input_shape=(64,64))(model_view)\n",
    "model_view = Activation('relu')(model_view)\n",
    "model_view = MaxPool1D(pool_size=(2, ), strides=(4,), padding='valid')(model_view)\n",
    "\n",
    "#model_view.compile()\n",
    "\n",
    "input2 = keras.layers.Input(shape=(1,))\n",
    "# equivalent to added = keras.layers.add([x1, x2])\n",
    "added = keras.layers.Add()([model_view, input2])\n",
    "\n",
    "out = keras.layers.Dense(len(possible_actions))(added)\n",
    "model = keras.models.Model(inputs=[input1, input2], outputs=out)\n",
    "model.summary()\n",
    "model.compile(loss='mse', optimizer=Adam(lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPISODES = 1000\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, model):\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = model\n",
    "\n",
    "   \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(possible_actions)\n",
    "        act_values = self.model.predict(state)\n",
    "        return possible_actions[np.argmax(act_values[0])]\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma *\n",
    "                          np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "FLAGS(['run_sc2'])\n",
    "\n",
    "viz = False\n",
    "save_replay = False\n",
    "steps_per_episode = 0 # 0 actually means unlimited\n",
    "MAX_EPISODES =35\n",
    "MAX_STEPS = 400\n",
    "steps = 0\n",
    "\n",
    "# create a map\n",
    "beacon_map = maps.get('MoveToBeacon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sc2_env.SC2Env(agent_race=None,\n",
    "                    bot_race=None,\n",
    "                    difficulty=None,\n",
    "                    map_name=beacon_map,\n",
    "                    visualize=viz,agent_interface_format=sc2_env.AgentInterfaceFormat(\n",
    "              feature_dimensions=sc2_env.Dimensions(\n",
    "                  screen=64,\n",
    "                  minimap=64))) as env :\n",
    "    agent = DQNAgent(model)\n",
    "    # agent.load(\"./save/cartpole-dqn.h5\")\n",
    "    done = False\n",
    "    batch_size = 32\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        obs = env.reset()\n",
    "        state = get_state(obs[0])\n",
    "        for time in range(500):\n",
    "            # env.render()\n",
    "            action = agent.act(state)\n",
    "            ##TO DO\n",
    "            \n",
    "            \n",
    "            #reward = reward if not done else -10\n",
    "            #next_state = np.reshape(next_state, [1, state_size])\n",
    "            #agent.remember(state, action, reward, next_state, done)\n",
    "            #state = next_state\n",
    "            #if done:\n",
    "             #   print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
    "              #        .format(e, EPISODES, time, agent.epsilon))\n",
    "              #  break\n",
    "            #if len(agent.memory) > batch_size:\n",
    "             #   agent.replay(batch_size)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
